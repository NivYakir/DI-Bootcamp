{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80abe0da",
   "metadata": {},
   "source": [
    "# Exercise 1: Defining the Problem and Data Collection for Loan Default Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a05ee84",
   "metadata": {},
   "source": [
    "1. Problem Statement\n",
    "- Develop a machine learning model to predict the likelihood of loan default by borrowers at the time of loan application, enabling the financial institution to make informed lending decisions and minimize credit risk.\n",
    "\n",
    "2. Specific Goals\n",
    "- Predict whether a loan applicant will default (binary classification: default vs. non-default)\n",
    "- Estimate the probability of default for risk-based pricing and decision-making\n",
    "- Identify key risk factors that contribute to loan defaults\n",
    "- Reduce financial losses from bad loans while maintaining responsible lending practices\n",
    "\n",
    "3. Identify and list the types of data you would need for this project\n",
    "- Applicant Personal Information (Age, Gender, Marital status, Number of dependents, Education level, Employment status and stability, Residential status (owner, renter, etc.), Geographic location (zip code, region))\n",
    "- Financial Information (Annual income, Employment history and job tenure, Other sources of income, Existing financial obligations, Savings and assets, Bank account details (checking/savings balances), Debt-to-income ratio)\n",
    "- Credit History (Credit score (FICO, VantageScore, etc.), Credit report history length, Number of credit accounts, Types o of credit (revolving, installment), Payment history on previous loans, Number of late payments and their severity, Bankruptcies, foreclosures, or collections, Credit utilization ratio, Recent credit inquiries)\n",
    "- Loan-Specific Information (Requested loan amount, Loan purpose (debt consolidation, home improvement, business, etc.), Loan term/duration, Interest rate, Collateral (if applicable), Loan-to-value ratio (for secured loans))\n",
    "\n",
    "4. Discuss the sources where you can collect this data\n",
    "- Institution's Loan Management System\n",
    "- Customer Relationship Management (CRM) System\n",
    "- Core Banking System\n",
    "- Credit Bureaus\n",
    "- Financial Data Aggregators\n",
    "- Public and Government Records"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c233dc",
   "metadata": {},
   "source": [
    "# Exercise 2: Feature Selection and Model Choice for Loan Default Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99baf4fe",
   "metadata": {},
   "source": [
    "Instructions: identify which features might be most relevant for predicting loan defaults (Ordered from Most to Least Important)\n",
    "- Applicant Income\n",
    "- Loan Amount / Load Amount Term\n",
    "- Credit History\n",
    "- Education\n",
    "- Property Area\n",
    "- Dependents / Marital Status"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe8b9d7",
   "metadata": {},
   "source": [
    "# Exercise 3: Training, Evaluating, and Optimizing the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aec148d",
   "metadata": {},
   "source": [
    "Which model(s) would you pick for a Loan Prediction ?\n",
    "\n",
    "Regression Model:\n",
    "- Highly interpretable (regulatory requirement in many jurisdictions)\n",
    "- Provides probability estimates naturally\n",
    "- Fast to train and deploy\n",
    "- Well-understood by business stakeholders and regulators\n",
    "- Coefficients can be directly explained in adverse action notices\n",
    "- Good for understanding linear relationships"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d22bc432",
   "metadata": {},
   "source": [
    "# Exercise 4: Designing Machine Learning Solutions for Specific Problems\n",
    "For each of these scenario, decide which type of machine learning would be most suitable. Explain."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78149718",
   "metadata": {},
   "source": [
    "1. Predicting Stock Prices : predict future prices\n",
    "- Supervised Learning; We know the 'labels' or features of each stock (historical prices) and we use that to determine the future price.\n",
    "2. Organizing a Library of Books : group books into genres or categories based on similarities.\n",
    "- Unsupervised Learning; We want to cluster the books depending on their similarities, this is a clear cut case for unsupervised learning.\n",
    "\n",
    "3. Program a robot to navigate and find the shortest path in a maze.\n",
    "- Reinforcement Learning; no labels, environmental feedback, goal oriented with delayed results, learns optimal policy through interaction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e51fa717",
   "metadata": {},
   "source": [
    "# Exercise 5 : Designing an Evaluation Strategy for Different ML Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dadf0fc",
   "metadata": {},
   "source": [
    "## Part 1: Supervised Learning Model - Random Forest Classifier (Fraud Detection)\n",
    "\n",
    "### Context: Binary classification to detect fraudulent transactions\n",
    "\n",
    "---\n",
    "\n",
    "### Evaluation Strategy:\n",
    "\n",
    "#### A. Performance Metrics\n",
    "\n",
    "**Primary Metrics:**\n",
    "\n",
    "**1. Precision**\n",
    "- Formula: TP / (TP + FP)\n",
    "- **Why important:** Minimizes false alarms (legitimate transactions flagged as fraud)\n",
    "- High false positives annoy customers and waste investigation resources\n",
    "- Target: >80%\n",
    "\n",
    "**2. Recall (Sensitivity)**\n",
    "- Formula: TP / (TP + FN)\n",
    "- **Why important:** Catch as many actual frauds as possible\n",
    "- Missing fraud (false negatives) leads to financial losses\n",
    "- Target: >70%\n",
    "\n",
    "**3. F1-Score**\n",
    "- Harmonic mean of precision and recall\n",
    "- **Why important:** Balances both metrics, especially useful for imbalanced data\n",
    "- Single metric to compare models\n",
    "\n",
    "**4. AUC-ROC (Area Under ROC Curve)**\n",
    "- **Why important:** Threshold-independent evaluation\n",
    "- Shows model's ability to distinguish between classes across all decision thresholds\n",
    "- Target: >0.85\n",
    "\n",
    "**5. AUC-PR (Precision-Recall Curve)**\n",
    "- **Why important:** More informative than ROC for highly imbalanced datasets\n",
    "- Fraud is typically <1% of transactions\n",
    "- Better reflects performance on minority class\n",
    "\n",
    "**Secondary Metrics:**\n",
    "\n",
    "**6. Confusion Matrix**\n",
    "- Visual breakdown of TP, TN, FP, FN\n",
    "- Helps understand error types\n",
    "\n",
    "**7. Specificity**\n",
    "- TN / (TN + FP)\n",
    "- Rate of correctly identifying legitimate transactions\n",
    "\n",
    "**8. Matthews Correlation Coefficient (MCC)**\n",
    "- Balanced metric even for imbalanced classes\n",
    "- Range: -1 to +1 (higher is better)\n",
    "\n",
    "---\n",
    "\n",
    "#### B. Validation Methods\n",
    "\n",
    "**1. Stratified K-Fold Cross-Validation (k=5 or 10)**\n",
    "- **Why:** Maintains class distribution in each fold (crucial for imbalanced data)\n",
    "- Provides robust performance estimates\n",
    "- Reduces variance in metrics\n",
    "- Detects overfitting\n",
    "\n",
    "**2. Hold-Out Validation Set**\n",
    "- 70% train, 15% validation, 15% test\n",
    "- Validation set for hyperparameter tuning\n",
    "- Test set touched only once for final evaluation\n",
    "\n",
    "**3. Time-Based Split (if temporal data)**\n",
    "- Train on older data, validate on recent data\n",
    "- Simulates real deployment scenario\n",
    "- Prevents data leakage\n",
    "\n",
    "**4. Class Imbalance Handling**\n",
    "- **Stratification** to maintain fraud ratio\n",
    "- Consider SMOTE or undersampling for training\n",
    "- Always evaluate on original class distribution\n",
    "\n",
    "---\n",
    "\n",
    "#### C. Additional Evaluation Techniques\n",
    "\n",
    "**1. ROC Curve Analysis**\n",
    "- Plot TPR vs FPR at various thresholds\n",
    "- Identify optimal operating point\n",
    "- Compare multiple models visually\n",
    "\n",
    "**2. Precision-Recall Curve**\n",
    "- More informative for imbalanced data\n",
    "- Shows precision-recall tradeoff\n",
    "- Find threshold that balances business needs\n",
    "\n",
    "**3. Threshold Optimization**\n",
    "- Default: 0.5\n",
    "- Optimize based on business cost:\n",
    "  - Cost of false positive (customer friction)\n",
    "  - Cost of false negative (fraud loss)\n",
    "- May use lower threshold (0.3) to catch more fraud\n",
    "\n",
    "**4. Feature Importance Analysis**\n",
    "- Identify most predictive features\n",
    "- Ensure model isn't using spurious correlations\n",
    "- Regulatory compliance and interpretability\n",
    "\n",
    "**5. Calibration Curves**\n",
    "- Check if predicted probabilities match actual frequencies\n",
    "- If model says 30% fraud probability, do 30% of those actually turn out fraudulent?\n",
    "\n",
    "---\n",
    "\n",
    "### Challenges and Limitations:\n",
    "\n",
    "**1. Class Imbalance**\n",
    "- Fraud might be <1% of data\n",
    "- **Challenge:** High accuracy (99%) can be achieved by always predicting \"not fraud\"\n",
    "- **Solution:** Focus on precision, recall, and AUC-PR instead of accuracy\n",
    "\n",
    "**2. Concept Drift**\n",
    "- Fraud patterns evolve over time\n",
    "- **Challenge:** Model trained on old data may not catch new fraud types\n",
    "- **Solution:** Regular retraining, monitoring performance degradation\n",
    "\n",
    "**3. Threshold Selection Subjectivity**\n",
    "- Different thresholds optimize different metrics\n",
    "- **Challenge:** Business must decide cost tradeoff between FP and FN\n",
    "- **Solution:** Involve stakeholders, calculate business cost explicitly\n",
    "\n",
    "**4. Data Leakage Risk**\n",
    "- Features that include future information\n",
    "- **Challenge:** Inflated performance estimates\n",
    "- **Solution:** Careful feature engineering, temporal validation\n",
    "\n",
    "**5. Limited Interpretability**\n",
    "- Random Forest is less interpretable than simpler models\n",
    "- **Challenge:** Difficult to explain decisions to customers or regulators\n",
    "- **Solution:** Use SHAP values, feature importance, have simpler fallback model\n",
    "\n",
    "**6. Evaluation vs. Production Mismatch**\n",
    "- Static test set vs. dynamic real-world environment\n",
    "- **Challenge:** Test performance may not reflect production performance\n",
    "- **Solution:** A/B testing, champion-challenger framework\n",
    "\n",
    "---\n",
    "\n",
    "## Part 2: Unsupervised Learning Model - K-Means Clustering (Customer Segmentation)\n",
    "\n",
    "### Context: Segment customers into groups for targeted marketing\n",
    "\n",
    "---\n",
    "\n",
    "### Evaluation Strategy:\n",
    "\n",
    "#### A. Internal Validation Metrics (No Ground Truth)\n",
    "\n",
    "**1. Silhouette Score**\n",
    "- **Range:** -1 to +1\n",
    "- **Interpretation:**\n",
    "  - +1: Perfect clustering (points close to own cluster, far from others)\n",
    "  - 0: Overlapping clusters\n",
    "  - -1: Wrong cluster assignment\n",
    "- **Why important:** Measures both cohesion and separation\n",
    "- Calculate per-sample and average across dataset\n",
    "- **Target:** >0.5 (good), >0.7 (excellent)\n",
    "\n",
    "**2. Elbow Method (Inertia/Within-Cluster Sum of Squares)**\n",
    "- Plot inertia vs. number of clusters\n",
    "- Look for \"elbow\" where improvement rate decreases\n",
    "- **Why important:** Helps choose optimal k\n",
    "- **Limitation:** Elbow not always clear, subjective interpretation\n",
    "\n",
    "**3. Davies-Bouldin Index**\n",
    "- **Range:** 0 to ∞ (lower is better)\n",
    "- Average ratio of within-cluster to between-cluster distances\n",
    "- **Why important:** Considers both compactness and separation\n",
    "- Less computationally expensive than silhouette\n",
    "\n",
    "**4. Calinski-Harabasz Index (Variance Ratio Criterion)**\n",
    "- **Range:** 0 to ∞ (higher is better)\n",
    "- Ratio of between-cluster to within-cluster variance\n",
    "- **Why important:** Higher values indicate better-defined clusters\n",
    "\n",
    "**5. Dunn Index**\n",
    "- Ratio of minimum inter-cluster distance to maximum intra-cluster distance\n",
    "- **Why important:** Rewards compact, well-separated clusters\n",
    "- **Limitation:** Sensitive to outliers\n",
    "\n",
    "---\n",
    "\n",
    "#### B. Stability and Robustness Checks\n",
    "\n",
    "**1. Clustering Stability**\n",
    "- Run algorithm multiple times with different initializations\n",
    "- Check if same clusters emerge\n",
    "- **Why important:** Good clustering should be reproducible\n",
    "\n",
    "**2. Subsample Stability**\n",
    "- Cluster subsets of data\n",
    "- Measure consistency with full dataset clustering\n",
    "- **Why important:** Clusters should be stable, not artifacts of specific sample\n",
    "\n",
    "**3. Perturbation Analysis**\n",
    "- Add small noise to data\n",
    "- Check if cluster assignments change drastically\n",
    "- **Why important:** Robust clusters shouldn't be overly sensitive to small changes\n",
    "\n",
    "---\n",
    "\n",
    "#### C. Domain-Specific Validation\n",
    "\n",
    "**1. Cluster Interpretation**\n",
    "- Examine cluster characteristics (mean values, distributions)\n",
    "- **Do clusters make business sense?**\n",
    "- Can we name/describe each segment meaningfully?\n",
    "\n",
    "**2. Cluster Size Distribution**\n",
    "- Check if clusters are reasonably sized\n",
    "- **Warning signs:**\n",
    "  - One cluster with 95% of data (not useful)\n",
    "  - Many tiny clusters (overfitting)\n",
    "\n",
    "**3. Feature Analysis per Cluster**\n",
    "- Compare feature distributions across clusters\n",
    "- Are clusters distinguishable on important dimensions?\n",
    "\n",
    "**4. Business Value Assessment**\n",
    "- Can marketing campaigns be differentiated per segment?\n",
    "- Do segments show different behaviors/preferences?\n",
    "- **Ultimate test:** Does clustering lead to actionable insights?\n",
    "\n",
    "---\n",
    "\n",
    "#### D. Comparative Evaluation\n",
    "\n",
    "**1. Compare Different k Values**\n",
    "```\n",
    "k=2: Silhouette=0.65, DBI=0.8\n",
    "k=3: Silhouette=0.71, DBI=0.6  ← Best\n",
    "k=4: Silhouette=0.68, DBI=0.7\n",
    "k=5: Silhouette=0.62, DBI=0.9\n",
    "```\n",
    "\n",
    "**2. Compare Different Algorithms**\n",
    "- K-Means vs. Hierarchical vs. DBSCAN\n",
    "- Different algorithms may reveal different structures\n",
    "\n",
    "**3. Gap Statistic**\n",
    "- Compare clustering quality to random data\n",
    "- **Why important:** Determines if structure exists at all\n",
    "- Tests if data actually has natural clusters\n",
    "\n",
    "---\n",
    "\n",
    "### Challenges and Limitations:\n",
    "\n",
    "**1. No Ground Truth**\n",
    "- **Challenge:** No \"correct answer\" to validate against\n",
    "- Unlike supervised learning, can't calculate accuracy\n",
    "- **Solution:** Use multiple metrics, domain expertise, business validation\n",
    "\n",
    "**2. Metric Disagreement**\n",
    "- Different metrics may suggest different optimal k\n",
    "- **Challenge:** Silhouette says k=3, elbow suggests k=5\n",
    "- **Solution:** Consider business context, not just mathematical optimum\n",
    "\n",
    "**3. Curse of Dimensionality**\n",
    "- Many features make distance metrics less meaningful\n",
    "- **Challenge:** Silhouette and distance-based metrics become unreliable in high dimensions\n",
    "- **Solution:** Dimensionality reduction (PCA) before clustering, feature selection\n",
    "\n",
    "**4. Subjectivity in Interpretation**\n",
    "- **Challenge:** Two analysts might interpret same clusters differently\n",
    "- What makes clusters \"good\" depends on use case\n",
    "- **Solution:** Clear business objectives, stakeholder involvement\n",
    "\n",
    "**5. Algorithm Assumptions**\n",
    "- K-Means assumes spherical clusters of similar size\n",
    "- **Challenge:** Real data may have irregular shapes\n",
    "- **Solution:** Try multiple algorithms, visual inspection\n",
    "\n",
    "**6. Local Optima**\n",
    "- K-Means sensitive to initialization\n",
    "- **Challenge:** Different runs give different results\n",
    "- **Solution:** Multiple random initializations, k-means++ initialization\n",
    "\n",
    "**7. Scalability of Validation Metrics**\n",
    "- Silhouette score O(n²) complexity\n",
    "- **Challenge:** Computationally expensive for large datasets\n",
    "- **Solution:** Sample-based approximation, use faster metrics like DBI\n",
    "\n",
    "**8. Determining \"True\" Number of Clusters**\n",
    "- **Challenge:** Real data may not have clear natural clustering\n",
    "- Continuous spectrum rather than discrete groups\n",
    "- **Solution:** Accept that k is somewhat arbitrary, choose based on business utility\n",
    "\n",
    "---\n",
    "\n",
    "## Part 3: Reinforcement Learning Model - Q-Learning Agent (Game Playing / Navigation)\n",
    "\n",
    "### Context: Agent learning to navigate maze or play game\n",
    "\n",
    "---\n",
    "\n",
    "### Evaluation Strategy:\n",
    "\n",
    "#### A. Performance Metrics\n",
    "\n",
    "**1. Cumulative Reward**\n",
    "- **Definition:** Total reward accumulated over episode\n",
    "- **Why important:** Primary objective of RL - maximize long-term reward\n",
    "- Track across training episodes to monitor improvement\n",
    "- **Analysis:**\n",
    "  - Mean cumulative reward\n",
    "  - Variance in rewards (consistency)\n",
    "  - Maximum achieved reward\n",
    "\n",
    "**2. Average Episode Reward**\n",
    "- Mean reward per episode over evaluation period\n",
    "- **Why important:** Normalized measure of performance\n",
    "- Compare across different evaluation windows\n",
    "\n",
    "**3. Success Rate**\n",
    "- Percentage of episodes where goal is achieved\n",
    "- **Why important:** Binary measure of task completion\n",
    "- Example: % of times agent reaches maze exit\n",
    "\n",
    "**4. Episode Length (Steps to Goal)**\n",
    "- Number of actions taken per episode\n",
    "- **Why important:** \n",
    "  - Shorter = more efficient policy\n",
    "  - Combined with success rate shows quality of solution\n",
    "- Track minimum, average, and maximum\n",
    "\n",
    "**5. Reward Per Step**\n",
    "- Average reward earned per action\n",
    "- **Why important:** Efficiency metric - achieving goals with less waste\n",
    "\n",
    "---\n",
    "\n",
    "#### B. Learning Progress Metrics\n",
    "\n",
    "**1. Learning Curve**\n",
    "- Plot cumulative/average reward vs. training episodes\n",
    "- **What to look for:**\n",
    "  - Upward trend (learning)\n",
    "  - Plateau (convergence)\n",
    "  - Fluctuations (exploration noise)\n",
    "\n",
    "**2. Convergence Analysis**\n",
    "- Has policy stabilized?\n",
    "- **Methods:**\n",
    "  - Moving average of rewards flattens\n",
    "  - Policy changes become minimal\n",
    "  - Q-value updates become small\n",
    "- **Why important:** Know when to stop training\n",
    "\n",
    "**3. Exploration Rate Decay**\n",
    "- Track epsilon (ε) in ε-greedy policy\n",
    "- **Why important:** Should decrease over time as agent gains experience\n",
    "- Verify exploration schedule is appropriate\n",
    "\n",
    "**4. Q-Value Evolution**\n",
    "- Monitor Q-value estimates for key state-action pairs\n",
    "- **Why important:** Should converge to stable values\n",
    "- Diverging Q-values indicate instability\n",
    "\n",
    "**5. Loss/TD Error**\n",
    "- Track temporal difference error over time\n",
    "- **Why important:** Should decrease as Q-values converge\n",
    "- Large persistent errors suggest learning problems\n",
    "\n",
    "---\n",
    "\n",
    "#### C. Policy Quality Assessment\n",
    "\n",
    "**1. Deterministic Evaluation**\n",
    "- Turn off exploration (ε=0), use greedy policy\n",
    "- **Why important:** See what agent actually learned, without random actions\n",
    "- Run multiple episodes to average out environment stochasticity\n",
    "\n",
    "**2. Policy Stability**\n",
    "- How much does policy change between evaluations?\n",
    "- **Why important:** Stable policy indicates convergence\n",
    "- Measure: percentage of state-action pairs that change\n",
    "\n",
    "**3. Optimal Trajectory Analysis**\n",
    "- Compare agent's path to known optimal solution (if available)\n",
    "- **Why important:** Benchmark against best possible performance\n",
    "- Measure deviation from optimality\n",
    "\n",
    "**4. State Coverage**\n",
    "- What percentage of state space has agent visited?\n",
    "- **Why important:** \n",
    "  - Poor coverage suggests insufficient exploration\n",
    "  - Some areas never learned\n",
    "- Visualize visited states (if low-dimensional)\n",
    "\n",
    "---\n",
    "\n",
    "#### D. Exploration vs. Exploitation Balance\n",
    "\n",
    "**1. Exploration Efficiency**\n",
    "- How quickly does agent discover high-reward states?\n",
    "- Time to first goal achievement\n",
    "- **Why important:** Measures exploration strategy effectiveness\n",
    "\n",
    "**2. Exploitation Verification**\n",
    "- Does agent consistently choose best known actions?\n",
    "- Compare greedy vs. ε-greedy performance\n",
    "- **Why important:** Ensure agent isn't over-exploring late in training\n",
    "\n",
    "**3. Regret Analysis**\n",
    "- Cumulative difference between optimal and achieved rewards\n",
    "- **Why important:** Quantifies cost of learning\n",
    "- Lower regret = better exploration-exploitation balance\n",
    "\n",
    "**4. Visit Count Distribution**\n",
    "- Frequency of visiting each state\n",
    "- **Why important:** \n",
    "  - Uniform = good exploration\n",
    "  - Concentrated = exploitation or poor exploration\n",
    "\n",
    "---\n",
    "\n",
    "#### E. Robustness and Generalization\n",
    "\n",
    "**1. Environment Variation Testing**\n",
    "- Test in slightly different environments\n",
    "- Add noise or perturbations\n",
    "- **Why important:** Agent shouldn't be overfitted to training environment\n",
    "\n",
    "**2. Different Initial States**\n",
    "- Start from various positions\n",
    "- **Why important:** Policy should work from any valid starting point\n",
    "\n",
    "**3. Transfer Learning Assessment**\n",
    "- Test on related but different tasks\n",
    "- **Why important:** Indicates if agent learned general principles\n",
    "\n",
    "**4. Adversarial Testing**\n",
    "- Introduce worst-case scenarios\n",
    "- **Why important:** Identify failure modes and edge cases\n",
    "\n",
    "---\n",
    "\n",
    "#### F. Comparison Baselines\n",
    "\n",
    "**1. Random Policy**\n",
    "- Agent taking random actions\n",
    "- **Why important:** Minimum baseline - learned policy must beat this\n",
    "\n",
    "**2. Heuristic/Rule-Based Policy**\n",
    "- Hand-crafted solution\n",
    "- **Why important:** Shows if RL provides value over simpler approaches\n",
    "\n",
    "**3. Optimal Policy (if computable)**\n",
    "- Known best solution\n",
    "- **Why important:** Upper bound on performance\n",
    "\n",
    "**4. Different RL Algorithms**\n",
    "- Compare Q-Learning vs. SARSA vs. DQN\n",
    "- **Why important:** Validate algorithm choice\n",
    "\n",
    "---\n",
    "\n",
    "### Challenges and Limitations:\n",
    "\n",
    "**1. Credit Assignment Problem**\n",
    "- **Challenge:** Long delay between action and reward\n",
    "- Hard to determine which actions were responsible for success/failure\n",
    "- **Impact on evaluation:** \n",
    "  - Slow learning manifests as flat learning curves\n",
    "  - Difficult to diagnose specific problems\n",
    "\n",
    "**2. Exploration-Exploitation Tradeoff**\n",
    "- **Challenge:** Too much exploration = suboptimal performance during training\n",
    "- Too little exploration = missing better strategies\n",
    "- **Impact on evaluation:**\n",
    "  - Training performance (with exploration) ≠ learned policy quality\n",
    "  - Must evaluate with exploration OFF for true assessment\n",
    "  - Choosing exploration schedule affects results\n",
    "\n",
    "**3. High Variance in Performance**\n",
    "- **Challenge:** RL training is inherently noisy\n",
    "- Stochastic environment and policy lead to variable results\n",
    "- **Impact on evaluation:**\n",
    "  - Single episode performance unreliable\n",
    "  - Need many episodes to get stable estimates\n",
    "  - Require multiple training runs with different seeds\n",
    "\n",
    "**4. Sample Efficiency**\n",
    "- **Challenge:** RL often requires millions of interactions\n",
    "- Expensive in real-world applications\n",
    "- **Impact on evaluation:**\n",
    "  - Long training times make iteration slow\n",
    "  - Difficult to do extensive hyperparameter search\n",
    "  - May need to evaluate before full convergence\n",
    "\n",
    "**5. Non-Stationary Learning**\n",
    "- **Challenge:** Target (optimal policy) changes as agent learns\n",
    "- Unlike supervised learning with fixed dataset\n",
    "- **Impact on evaluation:**\n",
    "  - Can't use traditional train/test split\n",
    "  - Early performance doesn't predict final performance\n",
    "  - Learning curves may show temporary degradation\n",
    "\n",
    "**6. Reward Engineering**\n",
    "- **Challenge:** Reward function dramatically affects learning\n",
    "- Poor rewards lead to unintended behaviors\n",
    "- **Impact on evaluation:**\n",
    "  - High reward doesn't always mean desired behavior\n",
    "  - Agent may exploit reward function\n",
    "  - Need to monitor actual behavior, not just rewards\n",
    "\n",
    "**7. Sparse Rewards**\n",
    "- **Challenge:** Reward only at goal, nothing during exploration\n",
    "- Agent may never discover reward signal\n",
    "- **Impact on evaluation:**\n",
    "  - Initial performance may be zero for long periods\n",
    "  - Difficult to distinguish \"not learned\" from \"can't learn\"\n",
    "  - May need reward shaping to evaluate learning progress\n",
    "\n",
    "**8. Partial Observability**\n",
    "- **Challenge:** Agent may not see full state\n",
    "- Memory/history needed for optimal decisions\n",
    "- **Impact on evaluation:**\n",
    "  - Performance ceiling limited by observability\n",
    "  - Hard to distinguish poor learning from fundamental limitations\n",
    "\n",
    "**9. Catastrophic Forgetting**\n",
    "- **Challenge:** Learning new skills may degrade previously learned ones\n",
    "- Especially in continual learning scenarios\n",
    "- **Impact on evaluation:**\n",
    "  - Performance on old tasks may decrease\n",
    "  - Need to monitor multiple metrics simultaneously\n",
    "\n",
    "**10. Evaluation Environment Mismatch**\n",
    "- **Challenge:** Evaluate in same environment used for training\n",
    "- May not reflect real-world performance\n",
    "- **Impact on evaluation:**\n",
    "  - Overfitting to training environment\n",
    "  - Need separate test environments\n",
    "\n",
    "**11. Reproducibility Issues**\n",
    "- **Challenge:** Sensitive to random seeds, initialization, hyperparameters\n",
    "- **Impact on evaluation:**\n",
    "  - Results may not replicate\n",
    "  - Need multiple runs with statistical testing\n",
    "  - Publication requires careful documentation\n",
    "\n",
    "**12. No Ground Truth Optimal Policy**\n",
    "- **Challenge:** Usually don't know theoretical optimal performance\n",
    "- **Impact on evaluation:**\n",
    "  - Can't calculate optimality gap\n",
    "  - Don't know how much improvement is possible\n",
    "  - Hard to know when to stop training\n",
    "\n",
    "---\n",
    "\n",
    "## Summary Comparison Table\n",
    "\n",
    "| Aspect | Supervised | Unsupervised | Reinforcement |\n",
    "|--------|------------|--------------|---------------|\n",
    "| **Ground Truth** | ✅ Yes (labels) | ❌ No | ⚠️ Indirect (rewards) |\n",
    "| **Primary Metric** | Accuracy/F1 | Silhouette/DBI | Cumulative Reward |\n",
    "| **Validation Method** | Train/Test Split | Internal metrics | Episode evaluation |\n",
    "| **Main Challenge** | Class imbalance | No correct answer | High variance |\n",
    "| **Evaluation Clarity** | High | Medium | Low |\n",
    "| **Interpretability** | Direct | Subjective | Behavior-based |\n",
    "\n",
    "---\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "Each model type requires fundamentally different evaluation approaches tailored to their unique characteristics and challenges. The key is to:\n",
    "\n",
    "1. **Supervised Learning:** Use multiple metrics beyond accuracy, especially for imbalanced data\n",
    "2. **Unsupervised Learning:** Combine mathematical metrics with domain knowledge and business value\n",
    "3. **Reinforcement Learning:** Monitor learning progress over time and evaluate with exploration disabled"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
