{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3ac0755",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import itertools\n",
    "import statsmodels.stats.api as sms\n",
    "from scipy.stats import (ttest_1samp, shapiro, levene, ttest_ind, mannwhitneyu,\n",
    "                         pearsonr, spearmanr, kendalltau, f_oneway, kruskal)\n",
    "from statsmodels.stats.proportion import proportions_ztest\n",
    "from statsmodels.stats.multicomp import MultiComparison\n",
    "import zipfile, requests, io\n",
    "from statsmodels.stats.power import NormalIndPower\n",
    "from statsmodels.stats.proportion import proportion_effectsize\n",
    "from statsmodels.stats.power import zt_ind_solve_power"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce8aacfc",
   "metadata": {},
   "source": [
    "# Exercise 1: Calculating Required Sample Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c29d916d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binary data (CTR): Required sample size per group = 2940 users\n",
      "Effect size (Cohen's h) = -0.073\n"
     ]
    }
   ],
   "source": [
    "# Inputs\n",
    "p1 = 0.2\n",
    "p2 = 0.23\n",
    "alpha = 0.05\n",
    "power = 0.8\n",
    "\n",
    "# Calculate Effect Size (Cohen's h for proportions)\n",
    "effect_size = proportion_effectsize(p1, p2)\n",
    "\n",
    "# Calculate sample size per group\n",
    "analysis = NormalIndPower()\n",
    "sample_size = analysis.solve_power(effect_size=effect_size, alpha=alpha, power=power, ratio=1)\n",
    "\n",
    "print(f\"Binary data (CTR): Required sample size per group = {int(sample_size)} users\")\n",
    "print(f\"Effect size (Cohen's h) = {effect_size:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4987dff",
   "metadata": {},
   "source": [
    "# Exercise 2: Understanding the Relationship Between Effect Size and Sample Size\n",
    "Calculate the required sample size for the following effect sizes: 0.2, 0.4, and 0.5, keeping the significance level and power the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d85ccdc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binary data (CTR): Required sample size per group = 6507 users\n",
      "Effect size (Cohen's h) = -0.049\n"
     ]
    }
   ],
   "source": [
    "# Effect Size = 0.2\n",
    "p1 = 0.2\n",
    "p2 = 0.22\n",
    "\n",
    "# Effect Size\n",
    "effect_size = proportion_effectsize(p1, p2)\n",
    "\n",
    "# Calculate Required Sample Size\n",
    "sample_size = analysis.solve_power(effect_size=effect_size, alpha=alpha, power=power, ratio=1)\n",
    "\n",
    "print(f\"Binary data (CTR): Required sample size per group = {int(sample_size)} users\")\n",
    "print(f\"Effect size (Cohen's h) = {effect_size:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a98b9fee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binary data (CTR): Required sample size per group = 1680 users\n",
      "Effect size (Cohen's h) = -0.097\n"
     ]
    }
   ],
   "source": [
    "# Effect Size = 0.4\n",
    "p1 = 0.2\n",
    "p2 = 0.24\n",
    "\n",
    "# Effect Size\n",
    "effect_size = proportion_effectsize(p1, p2)\n",
    "\n",
    "# Calculate Required Sample Size\n",
    "sample_size = analysis.solve_power(effect_size=effect_size, alpha=alpha, power=power, ratio=1)\n",
    "\n",
    "print(f\"Binary data (CTR): Required sample size per group = {int(sample_size)} users\")\n",
    "print(f\"Effect size (Cohen's h) = {effect_size:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d9d6db77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binary data (CTR): Required sample size per group = 1091 users\n",
      "Effect size (Cohen's h) = -0.120\n"
     ]
    }
   ],
   "source": [
    "# Effect Size = 0.5\n",
    "p1 = 0.2\n",
    "p2 = 0.25\n",
    "\n",
    "# Effect Size\n",
    "effect_size = proportion_effectsize(p1, p2)\n",
    "\n",
    "# Calculate Required Sample Size\n",
    "sample_size = analysis.solve_power(effect_size=effect_size, alpha=alpha, power=power, ratio=1)\n",
    "\n",
    "print(f\"Binary data (CTR): Required sample size per group = {int(sample_size)} users\")\n",
    "print(f\"Effect size (Cohen's h) = {effect_size:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e75ca7",
   "metadata": {},
   "source": [
    "**- How does the sample size change as the effect size increases? Explain why this happens.**\n",
    "\n",
    "**ANSWER:** As the expected effect size increases, the required sample size decreases. This is because it is much easier to observe large differences between two datasets, thus requiring a smaller sample size (less observations). The opposite holds true when you want to explore/detect minute differences; you need more observations to conclude with confidence that it isn't random noise. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8fe78d1",
   "metadata": {},
   "source": [
    "# Exercise 3: Exploring the Impact of Statistical Power\n",
    "Imagine you are conducting an A/B test where you expect a small effect size of 0.2. You initially plan for a power of 0.8 but wonder how increasing or decreasing the desired power level impacts the required sample size. Calculate the required sample size for power levels of 0.7, 0.8, and 0.9, keeping the effect size at 0.2 and significance level at 0.05."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a26998dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Required Sample Size per Group: 5117 observations\n",
      "Effect size (Cohen's h) = -0.049\n"
     ]
    }
   ],
   "source": [
    "# Power Level: 0.7\n",
    "p1 = 0.2\n",
    "p2 = 0.22\n",
    "alpha = 0.05\n",
    "power = 0.7\n",
    "\n",
    "effect_size = proportion_effectsize(p1, p2)\n",
    "sample_size = analysis.solve_power(effect_size=effect_size, alpha=alpha, power=power, ratio=1)\n",
    "\n",
    "print(f\"Required Sample Size per Group: {int(sample_size)} observations\")\n",
    "print(f\"Effect size (Cohen's h) = {effect_size:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0fd926eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Required Sample Size per Group: 6507 observations\n",
      "Effect size (Cohen's h) = -0.049\n"
     ]
    }
   ],
   "source": [
    "# Power Level: 0.8\n",
    "p1 = 0.2\n",
    "p2 = 0.22\n",
    "alpha = 0.05\n",
    "power = 0.8\n",
    "\n",
    "effect_size = proportion_effectsize(p1, p2)\n",
    "sample_size = analysis.solve_power(effect_size=effect_size, alpha=alpha, power=power, ratio=1)\n",
    "\n",
    "print(f\"Required Sample Size per Group: {int(sample_size)} observations\")\n",
    "print(f\"Effect size (Cohen's h) = {effect_size:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c896ca45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Required Sample Size per Group: 8711 observations\n",
      "Effect size (Cohen's h) = -0.049\n"
     ]
    }
   ],
   "source": [
    "# # Power Level: 0.7\n",
    "p1 = 0.2\n",
    "p2 = 0.22\n",
    "alpha = 0.05\n",
    "power = 0.9\n",
    "\n",
    "effect_size = proportion_effectsize(p1, p2)\n",
    "sample_size = analysis.solve_power(effect_size=effect_size, alpha=alpha, power=power, ratio=1)\n",
    "\n",
    "print(f\"Required Sample Size per Group: {int(sample_size)} observations\")\n",
    "print(f\"Effect size (Cohen's h) = {effect_size:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77e7a8ef",
   "metadata": {},
   "source": [
    "**How does the required sample size change with different levels of statistical power? Why is this understanding important when designing A/B tests?**\n",
    "\n",
    "**ANSWER**: As you increase the level of statistical power, the required sample size also increases. This is because Statistical power represents the probability that your test will detect an effect when that effect actually exists.\n",
    "In simpler terms, if there really IS a difference between your groups, what's the chance your test will successfully find it?\n",
    "\n",
    "Thus, as you increase the chance that a test will DETECT a difference as statistically significant, you will also need to increase the number of observations. More observations = more accurate results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05c66511",
   "metadata": {},
   "source": [
    "# Exercise 4: Implementing Sequential Testing\n",
    "You are running an A/B test on two versions of a product page to increase the purchase rate. You plan to monitor the results weekly and stop the test early if one version shows a significant improvement.\n",
    "- Define your stopping criteria.\n",
    "- Decide how you would implement sequential testing in this scenario.\n",
    "- At the end of week three, Version B has a p-value of 0.02. What would you do next?\n",
    "\n",
    "**ANSWER**: \n",
    "- The stopping criteria will be when the p-value is below 0.05\n",
    "- We would implement the sequential testing in the following way:\n",
    "    - At the end of each week, we would analyze the results of the A/B test. If at the end of the week, our analysis results in a p-value above 0.05, we will run the test for an additional week and analyze again. If in the following week we notice another decrease in the p-value, but it is still above our stopping criteria, we will repeat the process once more. As soon as we notice that the p-value is below 0.05, we can stop our testing and proceed with the appropriate business decision.\n",
    "- We would end our A/B testing as the a p-value of 0.05 is well below our threshold or stopping criteria."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb711b94",
   "metadata": {},
   "source": [
    "# Exercise 5: Applying Bayesian A/B Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1296acbc",
   "metadata": {},
   "source": [
    "You’re testing a new feature in your app, and you want to use a Bayesian approach. Initially, you believe the new feature has a 50% chance of improving user engagement. After collecting data, your analysis suggests a 65% probability that the new feature is better.\n",
    "\n",
    "- Describe how you would set up your prior belief.\n",
    "- After collecting data, how does the updated belief (posterior distribution) influence your decision?\n",
    "- What would you do if the posterior probability was only 55%?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86f9a831",
   "metadata": {},
   "source": [
    "**ANSWER**:\n",
    "- Our prior belief is that the app version with the new feature (Version B) has a 50% chance of being better than the the original version (Version A). In other words, we start with a neutral belief because there is a 50% one version could be better or worse than the other.\n",
    "- Our posterior distribution or belief is the probability that the new version is better than the original after analyzing the results of the test. In this case being our posterior is 65%. With our updated belief, we are now only 35% unsure that the new version is better, which isn't wholeheartedly convincing, but should be enough to attempt a small rollout or experimental phase. The key question is how confident we want to be before continuing with the new version.\n",
    "- If the posterior was only 55%, we would want to run additional tests to see if we can get a higher posterior to conclude that the new version is better."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f30dcb",
   "metadata": {},
   "source": [
    "# Exercise 6: Implementing Adaptive Experimentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd40eeee",
   "metadata": {},
   "source": [
    "You’re running a test with three different website layouts to increase user engagement. Initially, each layout gets 33% of the traffic. After the first week, Layout C shows higher engagement.\n",
    "\n",
    "- Explain how you would adjust the traffic allocation after the first week.\n",
    "- Describe how you would continue to adapt the experiment in the following weeks.\n",
    "- What challenges might you face with adaptive experimentation, and how would you address them?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37fbf81b",
   "metadata": {},
   "source": [
    "**ANSWER**:\n",
    "- After the first week, I would adjust the traffic to each layout in the following ways:\n",
    "    - 60% to C\n",
    "    - 20% to A\n",
    "    - 20% to B\n",
    "- If I notice that C continues to show higher engagement, I would divert more traffic there from layouts A and B, and potentially even phase out traffic to the lower performer between layouts A and B. From there I would eventually divert all traffic to the best performing layout.\n",
    "- The early analyses are open to noise, and we could potentially divert more traffic to a layout that isn't actually better than the others. There is also a higher chance for inconsistent results week over week. These issues can be addressed by increasing the sample size, running the test for more time, or making smaller adjustments week over week until a clearer pattern or result is observable. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyter_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
